Files already downloaded and verified
Files already downloaded and verified
 2021-12-01 03:24:22,451 | INFO | root 	 classes_order
 2021-12-01 03:24:22,452 | INFO | root 	 [[87, 0, 52, 58, 44, 91, 68, 97, 51, 15, 94, 92, 10, 72, 49, 78, 61, 14, 8, 86, 84, 96, 18, 24, 32, 45, 88, 11, 4, 67, 69, 66, 77, 47, 79, 93, 29, 50, 57, 83, 17, 81, 41, 12, 37, 59, 25, 20, 80, 73, 1, 28, 6, 46, 62, 82, 53, 9, 31, 75, 38, 63, 33, 74, 27, 22, 36, 3, 16, 21, 60, 19, 70, 90, 89, 43, 5, 42, 65, 76, 40, 30, 23, 85, 2, 95, 56, 48, 71, 64, 98, 13, 99, 7, 34, 55, 54, 26, 35, 39]]
 2021-12-01 03:24:25,234 | INFO | root 	 Begin step 0
 2021-12-01 03:24:25,234 | INFO | root 	 Now [200, 200, 200, 200, 200, 200, 200, 200, 200, 200] examplars per class.
 2021-12-01 03:24:25,235 | INFO | root 	 Step 0 weight decay 0.00050
 2021-12-01 03:24:25,319 | INFO | root 	 Train on 0->10.
 2021-12-01 03:24:25,320 | INFO | root 	 nb 5000
 2021-12-01 03:24:25,321 | INFO | root 	 Initial trainset: Weight norm per class [1.007]
warmup
 2021-12-01 03:24:27,276 | INFO | root 	 Initial trainset: Feature norm per class [1.394]
 2021-12-01 03:24:29,880 | INFO | root 	 Task 1/10, Epoch 1/17 => Clf loss: 2.037 Aux loss: 0.0, Train Accu: 28.26
 2021-12-01 03:24:33,159 | INFO | root 	 Task 1/10, Epoch 2/17 => Clf loss: 1.783 Aux loss: 0.0, Train Accu: 37.38
 2021-12-01 03:24:36,506 | INFO | root 	 Task 1/10, Epoch 3/17 => Clf loss: 1.688 Aux loss: 0.0, Train Accu: 39.8
 2021-12-01 03:24:39,837 | INFO | root 	 Task 1/10, Epoch 4/17 => Clf loss: 1.636 Aux loss: 0.0, Train Accu: 42.26
 2021-12-01 03:24:43,141 | INFO | root 	 Task 1/10, Epoch 5/17 => Clf loss: 1.525 Aux loss: 0.0, Train Accu: 46.56
 2021-12-01 03:24:46,440 | INFO | root 	 Task 1/10, Epoch 6/17 => Clf loss: 1.465 Aux loss: 0.0, Train Accu: 49.02
 2021-12-01 03:24:49,747 | INFO | root 	 Task 1/10, Epoch 7/17 => Clf loss: 1.406 Aux loss: 0.0, Train Accu: 51.38
 2021-12-01 03:24:53,102 | INFO | root 	 Task 1/10, Epoch 8/17 => Clf loss: 1.331 Aux loss: 0.0, Train Accu: 55.04
 2021-12-01 03:24:56,493 | INFO | root 	 Task 1/10, Epoch 9/17 => Clf loss: 1.179 Aux loss: 0.0, Train Accu: 59.9
 2021-12-01 03:24:59,891 | INFO | root 	 Task 1/10, Epoch 10/17 => Clf loss: 1.113 Aux loss: 0.0, Train Accu: 61.36
 2021-12-01 03:25:03,248 | INFO | root 	 Task 1/10, Epoch 11/17 => Clf loss: 1.26 Aux loss: 0.0, Train Accu: 56.58
 2021-12-01 03:25:06,541 | INFO | root 	 Task 1/10, Epoch 12/17 => Clf loss: 0.995 Aux loss: 0.0, Train Accu: 66.08
 2021-12-01 03:25:09,885 | INFO | root 	 Task 1/10, Epoch 13/17 => Clf loss: 0.912 Aux loss: 0.0, Train Accu: 68.26
 2021-12-01 03:25:13,219 | INFO | root 	 Task 1/10, Epoch 14/17 => Clf loss: 0.932 Aux loss: 0.0, Train Accu: 67.68
 2021-12-01 03:25:16,522 | INFO | root 	 Task 1/10, Epoch 15/17 => Clf loss: 0.803 Aux loss: 0.0, Train Accu: 71.6
 2021-12-01 03:25:19,893 | INFO | root 	 Task 1/10, Epoch 16/17 => Clf loss: 0.711 Aux loss: 0.0, Train Accu: 76.96
 2021-12-01 03:25:23,213 | INFO | root 	 Task 1/10, Epoch 17/17 => Clf loss: 0.812 Aux loss: 0.0, Train Accu: 72.92
 2021-12-01 03:25:23,977 | INFO | root 	 After training: Weight norm per class [2.314]
ema init
 2021-12-01 03:25:25,359 | INFO | root 	 Trainset: Feature norm per class [5.462]
 2021-12-01 03:25:27,364 | INFO | root 	 build memory
 2021-12-01 03:25:27,364 | INFO | root 	 Building & updating memory.(iCaRL)
Set memory of size: 2000.
warmup
 2021-12-01 03:25:36,519 | INFO | root 	 Save step0 memory!
 2021-12-01 03:25:36,520 | INFO | root 	 Eval on 0->10.
 2021-12-01 03:25:37,311 | INFO | root 	 top1:{'total': 71.6, '00-09': 71.6}
 2021-12-01 03:25:37,312 | INFO | root 	 top1 ema:{'total': 71.6, '00-09': 71.6}
 2021-12-01 03:25:37,312 | INFO | root 	 top5:{'total': 97.7, '00-09': 97.7}
 2021-12-01 03:25:37,312 | INFO | root 	 top5 ema:{'total': 97.7, '00-09': 97.7}
 2021-12-01 03:25:37,694 | INFO | root 	 Begin step 1
 2021-12-01 03:25:37,694 | INFO | root 	 Now [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] examplars per class.
 2021-12-01 03:25:37,696 | INFO | root 	 Step 1 weight decay 0.00050
 2021-12-01 03:25:37,785 | INFO | root 	 Train on 10->20.
 2021-12-01 03:25:37,785 | INFO | root 	 nb 7000
 2021-12-01 03:25:37,787 | INFO | root 	 Initial trainset: Weight norm per class [1.009, 0.998]
> /home/share/jiawei/solf/inclearn/models/incmodel.py(510)_get_fake_minibatch()
-> Z_res[Y==i] = torch.mm(Z_hat,self._Ws[i])
[?2004h(Pdb)
 2021-12-01 03:25:39,282 | INFO | root 	 Initial trainset: Feature norm per class [5.419, 4.948]
Traceback (most recent call last):
  File "/home/share/jiawei/anaconda3/envs/torch3090/lib/python3.7/site-packages/sacred/experiment.py", line 318, in run_commandline
    options=args,
  File "/home/share/jiawei/anaconda3/envs/torch3090/lib/python3.7/site-packages/sacred/experiment.py", line 276, in run
    run()
  File "/home/share/jiawei/anaconda3/envs/torch3090/lib/python3.7/site-packages/sacred/run.py", line 238, in __call__
    self.result = self.main_function(*args)
  File "/home/share/jiawei/anaconda3/envs/torch3090/lib/python3.7/site-packages/sacred/config/captured_function.py", line 42, in captured_function
    result = wrapped(*args, **kwargs)
  File "/home/share/jiawei/solf/codes/base/main.py", line 69, in train
    _train(cfg, _run, ex, tensorboard)
  File "/home/share/jiawei/solf/codes/base/main.py", line 120, in _train
    model.train_task(train_loader, val_loader)
  File "/home/share/jiawei/solf/inclearn/models/base.py", line 43, in train_task
    self._train_task(train_loader, val_loader)
  File "/home/share/jiawei/solf/inclearn/models/incmodel.py", line 283, in _train_task
    old_accu=train_old_accu,
  File "/home/share/jiawei/solf/inclearn/models/incmodel.py", line 378, in _forward_loss
    loss, aux_loss = self._compute_loss(inputs, targets, [outputs, ema_outputs], accu)
  File "/home/share/jiawei/solf/inclearn/models/incmodel.py", line 393, in _compute_loss
    Z_fake, Y_fake = self._get_fake_minibatch()
  File "/home/share/jiawei/solf/inclearn/models/incmodel.py", line 510, in _get_fake_minibatch
    Z_res[Y==i] = torch.mm(Z_hat,self._Ws[i])
[?2004h(Pdb) [?2004l
